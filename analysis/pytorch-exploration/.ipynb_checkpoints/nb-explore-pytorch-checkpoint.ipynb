{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import tensorflow as tf\n",
    "from statsmodels.discrete.discrete_model import NegativeBinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils.sparsefuncs import inplace_row_scale\n",
    "import torch\n",
    "\n",
    "sc.set_figure_params(figsize=(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cudnn.enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eb_adata = sc.read_h5ad(\"/project2/gilad/jpopp/ebQTL/data/single_cell_objects/Lowpass.3seqbatches.merged.TEMP.filtered.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = eb_adata.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = pd.get_dummies(eb_adata.obs['individual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_llik(x, mean, inv_disp):\n",
    "    \"\"\" Return the log likelihood of x distributed as NB\n",
    "    See Hilbe 2012, eq. 8.10\n",
    "    mean - mean (> 0)\n",
    "    inv_disp - inverse dispersion (> 0)\n",
    "    \"\"\"\n",
    "    assert x.shape == mean.shape == inv_disp.shape\n",
    "    return (x * tf.log(mean / inv_disp) -\n",
    "          x * tf.log(1 + mean / inv_disp) -\n",
    "          inv_disp * tf.log(1 + mean / inv_disp) +\n",
    "          tf.lgamma(x + inv_disp) -\n",
    "          tf.lgamma(inv_disp) -\n",
    "          tf.lgamma(x + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "umi = counts.todense()\n",
    "size_factor = counts.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "design = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot create a tensor proto whose content is larger than 2GB.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mas_default(), graph\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/gpu:*\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      9\u001b[0m     size_factor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mVariable(size_factor, trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 10\u001b[0m     umi \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVariable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mumi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     onehot \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mVariable(onehot, trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m design \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/midway2/jpopp/.conda/envs/tf-scanpy/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/scratch/midway2/jpopp/.conda/envs/tf-scanpy/lib/python3.10/site-packages/tensorflow/python/framework/tensor_util.py:525\u001b[0m, in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_same_size \u001b[38;5;129;01mand\u001b[39;00m numpy_dtype \u001b[38;5;129;01min\u001b[39;00m _TENSOR_CONTENT_TYPES \u001b[38;5;129;01mand\u001b[39;00m shape_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    524\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m nparray\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m*\u001b[39m nparray\u001b[38;5;241m.\u001b[39mitemsize \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<<\u001b[39m \u001b[38;5;241m31\u001b[39m):\n\u001b[0;32m--> 525\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot create a tensor proto whose content is larger than 2GB.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    527\u001b[0m   tensor_proto\u001b[38;5;241m.\u001b[39mtensor_content \u001b[38;5;241m=\u001b[39m nparray\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    528\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tensor_proto\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot create a tensor proto whose content is larger than 2GB."
     ]
    }
   ],
   "source": [
    "assert onehot.shape[0] == umi.shape[0]\n",
    "if design is not None:\n",
    "    assert onehot.shape[0] == design.shape[0]\n",
    "n, p = umi.shape\n",
    "_, m = onehot.shape\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default(), graph.device('/gpu:*'):\n",
    "    size_factor = tf.Variable(size_factor, trainable=False)\n",
    "    umi = tf.Variable(umi, trainable=False)\n",
    "    onehot = tf.Variable(onehot, trainable=False)\n",
    "    if design is not None:\n",
    "        _, k = design.shape\n",
    "        design = tf.Variable(design, trainable=False)\n",
    "        beta = tf.Variable(tf.zeros([k, p]))\n",
    "\n",
    "    mean = tf.exp(tf.Variable(tf.zeros([m, p])))\n",
    "    inv_disp = tf.exp(tf.Variable(tf.zeros([m, p])))\n",
    "    logodds = tf.Variable(tf.constant(-8.0, shape=[m, p])) \n",
    "    lam = size_factor * tf.matmul(onehot, mean)\n",
    "    if design is not None:\n",
    "        lam *= tf.exp(tf.matmul(design, beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(umi, onehot, size_factor, design=None, learning_rate=1e-2,\n",
    "        max_epochs=100000, return_beta=False, warm_start=None, verbose=False):\n",
    "  \"\"\"Return the maximum likelihood solution of the model.\n",
    "  If warm_start is not given, optimize the negative binomial likelihood only to\n",
    "  find a good initialization. That problem is convex, so the initialization\n",
    "  does not matter.\n",
    "  umi - count matrix (n x p; float32)\n",
    "  onehot - mapping of individuals to cells (m x n; float32)\n",
    "  size_factor - size factor vector (n x 1; float32)\n",
    "  design - confounder matrix (n x q; float32)\n",
    "  learning_rate - step size for gradient descent\n",
    "  max_epochs - maximum number of gradient descent steps\n",
    "  return_beta - if True, returns the estimated confounding effect size matrix\n",
    "  verbose - if True, outputs the log likelihood every 500 epochs\n",
    "  warm_start - tuple of (log_mean, log_disp, logodds)\n",
    "  Returns:\n",
    "  log_mean - log mean parameter (m x p)\n",
    "  log_disp - log dispersion parameter (m x p)\n",
    "  logodds - logit proportion of excess zeros (m x p)\n",
    "  nb_nll - negative binomial negative log likelihood at the returned solution\n",
    "  zinb_nll - zero-inflated negative binomial negative log likelihood at the returned solution\n",
    "  beta - if return_beta, confounding effect size matrix (q x p)\n",
    "  \"\"\"\n",
    "  assert onehot.shape[0] == umi.shape[0]\n",
    "  if design is not None:\n",
    "    assert onehot.shape[0] == design.shape[0]\n",
    "  n, p = umi.shape\n",
    "  _, m = onehot.shape\n",
    "\n",
    "  graph = tf.Graph()\n",
    "  with graph.as_default(), graph.device('/gpu:*'):\n",
    "    size_factor = tf.Variable(size_factor, trainable=False)\n",
    "    umi = tf.Variable(umi, trainable=False)\n",
    "    onehot = tf.Variable(onehot, trainable=False)\n",
    "    if design is not None:\n",
    "      _, k = design.shape\n",
    "      design = tf.Variable(design, trainable=False)\n",
    "      beta = tf.Variable(tf.zeros([k, p]))\n",
    "\n",
    "    if warm_start is not None:\n",
    "      log_mean, log_disp, logodds = warm_start\n",
    "      assert log_mean.shape == (m, p)\n",
    "      assert log_disp.shape == (m, p)\n",
    "      assert logodds.shape == (m, p)\n",
    "      mean = tf.exp(tf.Variable(log_mean))\n",
    "      inv_disp = tf.exp(tf.Variable(-log_disp))\n",
    "      logodds = tf.Variable(logodds)\n",
    "    else:\n",
    "      mean = tf.exp(tf.Variable(tf.zeros([m, p])))\n",
    "      inv_disp = tf.exp(tf.Variable(tf.zeros([m, p])))\n",
    "      logodds = tf.Variable(tf.constant(-8.0, shape=[m, p]))\n",
    "\n",
    "    lam = size_factor * tf.matmul(onehot, mean)\n",
    "    if design is not None:\n",
    "      lam *= tf.exp(tf.matmul(design, beta))\n",
    "\n",
    "    nb_nll = -tf.reduce_sum(nb_llik(umi, lam, tf.matmul(onehot, inv_disp)))\n",
    "    zinb_nll = -tf.reduce_sum(zinb_llik(umi, lam, tf.matmul(onehot, inv_disp), tf.matmul(onehot, logodds)))\n",
    "\n",
    "    train_nb = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(nb_nll)\n",
    "    train_zinb = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(zinb_nll)\n",
    "\n",
    "    opt = [tf.log(mean), -tf.log(inv_disp), logodds, nb_nll, zinb_nll]\n",
    "    if return_beta:\n",
    "      opt.append(beta)\n",
    "\n",
    "    obj = float('-inf')\n",
    "    with tf.Session() as sess:\n",
    "      sess.run(tf.global_variables_initializer())\n",
    "      for i in range(max_epochs):\n",
    "        if warm_start is None:\n",
    "          _, update = sess.run([train_nb, nb_nll])\n",
    "        else:\n",
    "          _, update = sess.run([train_zinb, zinb_nll])          \n",
    "        if not np.isfinite(update):\n",
    "          raise tf.train.NanLossDuringTrainingError\n",
    "        if verbose and not i % 500:\n",
    "          print(i, update, end='\\r')\n",
    "      if verbose:\n",
    "        print(i, update)\n",
    "      return sess.run(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to use scVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eb_adata = sc.read_h5ad(\"/project2/gilad/jpopp/ebQTL/data/single_cell_objects/Lowpass.3seqbatches.merged.1ksubsampled.raw.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.highly_variable_genes(eb_adata, n_top_genes=3000, flavor='seurat_v3', subset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m Using batches from adata.obs\u001b[1m[\u001b[0m\u001b[32m\"library.prep.batch\"\u001b[0m\u001b[1m]\u001b[0m                                  \n",
      "\u001b[34mINFO    \u001b[0m No label_key inputted, assuming all cells have same label                           \n",
      "\u001b[34mINFO    \u001b[0m Using data from adata.X                                                             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/midway2/jpopp/.conda/envs/scvi-scanpy/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function setup_anndata is deprecated; Please use the model-specific setup_anndata methods instead. The global method will be removed in version 0.15.0.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m Successfully registered anndata object containing \u001b[1;36m16669\u001b[0m cells, \u001b[1;36m3000\u001b[0m vars, \u001b[1;36m4\u001b[0m batches,\n",
      "         \u001b[1;36m1\u001b[0m labels, and \u001b[1;36m0\u001b[0m proteins. Also registered \u001b[1;36m1\u001b[0m extra categorical covariates and \u001b[1;36m0\u001b[0m extra\n",
      "         continuous covariates.                                                              \n",
      "\u001b[34mINFO    \u001b[0m Please do not further modify adata until model is trained.                          \n"
     ]
    }
   ],
   "source": [
    "scvi.data.setup_anndata(eb_adata, batch_key=\"library.prep.batch\",\n",
    "                        categorical_covariate_keys=['Collection.Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = scvi.model.LinearSCVI(eb_adata, n_layers=1, n_latent=0, gene_likelihood='nb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Set SLURM handle signals.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/400:  10%|â–‰         | 39/400 [05:02<46:08,  7.67s/it, loss=1.36e+03, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/midway2/jpopp/.conda/envs/scvi-scanpy/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:897: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn('Detected KeyboardInterrupt, attempting graceful shutdown...')\n"
     ]
    }
   ],
   "source": [
    "vae.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scvi-scanpy",
   "language": "python",
   "name": "scvi-scanpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
